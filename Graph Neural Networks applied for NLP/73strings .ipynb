{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my solution to the question given as case-study. I will briefly describe the overall procedure initially and also include explanation as docstrings wherever needed. The problem required me to classify publicly-listed companies according to their descriptions. The GNN model was a necessity and hence I had to go through a couple of research papers before attempting to solve it. My approach is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation\n",
    "1. Pre-process the data and tokenize it\n",
    "2. Convert the text and labels into a graph using the approach given in \"Graph Convolutional Networks for Text Classification\" by Liang Yao, Chengsheng Mao and Yuan Luo. We create an adjacency matrix out of the text data using the following rules. A(i, j) is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                    PMI(i, j)     i, j are words, PMI(i, j) > 0\n",
    "                                                    TF-IDF(ij)    i is document, j is word\n",
    "                                                    1             i = j\n",
    "                                                    0             otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After creating the graph using networkx, we then define our GCN model. In this case I have used a 2-layer model.\n",
    "4. Finally we train the GCN, with the Classification tags as our outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "I had to run this notebook on Google cloud by creating a VM instance of AI notebooks as Google Colab and Kaggle both have RAM limits of 13 GB (being free) which weren't enough when the notebook reached the graph making step. \n",
    "Hence I used a VM with 8 vCPU's and 64 GB RAM. Even after this, the VM has crashed every time at 40% of the graph plotting being completed with error 524. Though I haven't been able to complete the task, given a bit more time and resources I could surely complete it as I have gone through multiple research papers to grasp what needs to be done logically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import OrderedDict\n",
    "from itertools import combinations\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Business Description</th>\n",
       "      <th>Industry Classification Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADSOUTH PARTNERS, INC.</td>\n",
       "      <td>Adsouth Partners, Inc. provides advertising ag...</td>\n",
       "      <td>Advertising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Artec Global Media, Inc.</td>\n",
       "      <td>Artec Global Media, Inc., formerly Artec Consu...</td>\n",
       "      <td>Advertising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Betawave Corp.</td>\n",
       "      <td>Betawave Corporation provides online marketing...</td>\n",
       "      <td>Advertising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BOSTON OMAHA Corp</td>\n",
       "      <td>Boston Omaha Corporation is engaged in the bus...</td>\n",
       "      <td>Advertising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bright Mountain Media Inc</td>\n",
       "      <td>Bright Mountain Media, Inc. is a digital media...</td>\n",
       "      <td>Advertising</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Company Name  \\\n",
       "0     ADSOUTH PARTNERS, INC.   \n",
       "1   Artec Global Media, Inc.   \n",
       "2             Betawave Corp.   \n",
       "3          BOSTON OMAHA Corp   \n",
       "4  Bright Mountain Media Inc   \n",
       "\n",
       "                                Business Description  \\\n",
       "0  Adsouth Partners, Inc. provides advertising ag...   \n",
       "1  Artec Global Media, Inc., formerly Artec Consu...   \n",
       "2  Betawave Corporation provides online marketing...   \n",
       "3  Boston Omaha Corporation is engaged in the bus...   \n",
       "4  Bright Mountain Media, Inc. is a digital media...   \n",
       "\n",
       "  Industry Classification Tag  \n",
       "0                 Advertising  \n",
       "1                 Advertising  \n",
       "2                 Advertising  \n",
       "3                 Advertising  \n",
       "4                 Advertising  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Training_Data.01 (1).csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company Name                   object\n",
       "Business Description           object\n",
       "Industry Classification Tag    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.astype(str)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Tf-idf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "  0%|          | 2/6045 [00:00<09:05, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating co-occurences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6045/6045 [03:34<00:00, 28.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48760\n"
     ]
    }
   ],
   "source": [
    "def word_word_edges(p_ij):\n",
    "    word_word = []\n",
    "    cols = list(p_ij.columns); cols = [str(w) for w in cols]\n",
    "    for w1, w2 in tqdm(combinations(cols, 2), total=nCr(len(cols), 2)):\n",
    "        if (p_ij.loc[w1,w2] > 0):\n",
    "            word_word.append((w1,w2,{\"weight\":p_ij.loc[w1,w2]}))\n",
    "    return word_word\n",
    "\n",
    "def nCr(n,r):\n",
    "    f = math.factorial\n",
    "    return int(f(n)/(f(r)*f(n-r)))\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "stopwords = list(set(nltk.corpus.stopwords.words(\"english\")))\n",
    "\n",
    "# remove stopwords and non-words from tokens list\n",
    "def filter_tokens(tokens, stopwords):\n",
    "    tokens1 = []\n",
    "    for token in tokens:\n",
    "        if (token not in stopwords) and (token not in [\".\",\",\",\";\",\"&\",\"'s\", \":\", \"?\", \"!\",\"(\",\")\",\\\n",
    "            \"'\",\"'m\",\"'no\",\"***\",\"--\",\"...\",\"[\",\"]\"]):\n",
    "            tokens1.append(token)\n",
    "    return tokens1\n",
    "\n",
    "# Company name isn't important for our problem\n",
    "df.drop([\"Company Name\"], axis=1, inplace=True)\n",
    "    \n",
    "# tokenize & remove funny characters\n",
    "df[\"Business Description\"] = df[\"Business Description\"].apply(lambda x: nltk.word_tokenize(x)).apply(lambda x: filter_tokens(x, stopwords))\n",
    "    \n",
    "# Tf-idf\n",
    "print(\"Calculating Tf-idf...\")\n",
    "vectorizer = TfidfVectorizer(input=\"content\", max_features=None, tokenizer=dummy_fun, preprocessor=dummy_fun)\n",
    "vectorizer.fit(df[\"Business Description\"])\n",
    "df_tfidf = vectorizer.transform(df[\"Business Description\"])\n",
    "df_tfidf = df_tfidf.toarray()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab = np.array(vocab)\n",
    "df_tfidf = pd.DataFrame(df_tfidf,columns=vocab)\n",
    "    \n",
    "# PMI between words\n",
    "names = vocab\n",
    "n_i  = OrderedDict((name, 0) for name in names)\n",
    "word2index = OrderedDict( (name,index) for index,name in enumerate(names) )\n",
    "\n",
    "occurrences = np.zeros( (len(names),len(names)) ,dtype=np.int32)\n",
    "# Find the co-occurrences:\n",
    "no_windows = 0\n",
    "print(\"Calculating co-occurences...\")\n",
    "window = 10\n",
    "for l in tqdm(df[\"Business Description\"], total=len(df[\"Business Description\"])):\n",
    "    for i in range(len(l)-window):\n",
    "        no_windows += 1\n",
    "        d = set(l[i:(i+window)])\n",
    "\n",
    "        for w in d:\n",
    "            n_i[w] += 1\n",
    "        for w1,w2 in combinations(d,2):\n",
    "            i1 = word2index[w1]\n",
    "            i2 = word2index[w2]\n",
    "\n",
    "            occurrences[i1][i2] += 1\n",
    "            occurrences[i2][i1] += 1\n",
    "\n",
    "### convert to PMI\n",
    "p_ij = pd.DataFrame(occurrences, index = names,columns=names)/no_windows\n",
    "p_i = pd.Series(n_i, index=n_i.keys())/no_windows\n",
    "\n",
    "del occurrences\n",
    "del n_i\n",
    "\n",
    "for col in p_ij.columns:\n",
    "    p_ij[col] = p_ij[col]/p_i[col]\n",
    "    \n",
    "for row in p_ij.index:\n",
    "    p_ij.loc[row,:] = p_ij.loc[row,:]/p_i[row]\n",
    "    \n",
    "p_ij = p_ij + 1E-9\n",
    "flag = 0\n",
    "for col in p_ij.columns:\n",
    "    p_ij[col] = np.log(p_ij[col])\n",
    "    flag += 1\n",
    "print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building graph (No. of document, word nodes: 6045, 48760)...\n",
      "Adding document nodes to graph...\n",
      "Adding word nodes to graph...\n",
      "Building document-word edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1999/6045 [26:38<53:56,  1.25it/s]  \n",
      "  0%|          | 1439/1188744420 [00:00<22:57:07, 14386.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "Building word-word edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 88852806/1188744420 [23:30<4:51:03, 62983.71it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a3710e8b99c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building word-word edges...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mword_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_word_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_ij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Adding document-word and word-word edges...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edges_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-02dc1ac9397a>\u001b[0m in \u001b[0;36mword_word_edges\u001b[0;34m(p_ij)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_ij\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnCr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_ij\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mword_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp_ij\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   3127\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3130\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3783\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3784\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3786\u001b[0m             \u001b[0;31m# All places that call _get_item_cache have unique columns,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Build graph\n",
    "print(\"Building graph (No. of document, word nodes: %d, %d)...\" %(len(df_tfidf.index), len(vocab)))\n",
    "G = nx.Graph()\n",
    "print(\"Adding document nodes to graph...\")\n",
    "G.add_nodes_from(df_tfidf.index) ## document nodes\n",
    "print(\"Adding word nodes to graph...\")\n",
    "G.add_nodes_from(vocab) ## word nodes\n",
    "### build edges between document-word pairs\n",
    "print(\"Building document-word edges...\")\n",
    "\n",
    "document_word = []\n",
    "flag = 0\n",
    "for doc in tqdm(df_tfidf.index, total=len(df_tfidf.index)):\n",
    "    flag += 1\n",
    "    for w in df_tfidf.columns:\n",
    "        document_word.append((doc,w,{\"weight\":df_tfidf.loc[doc,w]}))\n",
    "    if flag == 2000:                    # break at 2000 iterations as\n",
    "        break\n",
    "print(flag)\n",
    "    \n",
    "print(\"Building word-word edges...\")\n",
    "word_word = word_word_edges(p_ij)\n",
    "print(\"Adding document-word and word-word edges...\")\n",
    "G.add_edges_from(document_word)\n",
    "G.add_edges_from(word_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, with_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layered GCN\n",
    "# We are going to use a two-layer GCN(features are convolved twice) here as, according to the paper, it gives the best results. \n",
    "# The convoluted output feature tensor after the two-layer GCN is given by\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class gcn(nn.Module):\n",
    "    def __init__(self, X_size, A_hat, args, bias=True): # X_size = num features\n",
    "        super(gcn, self).__init__()\n",
    "        self.A_hat = torch.tensor(A_hat, requires_grad=False).float()\n",
    "        self.weight = nn.parameter.Parameter(torch.FloatTensor(X_size, args.hidden_size_1))\n",
    "        var = 2./(self.weight.size(1)+self.weight.size(0))\n",
    "        self.weight.data.normal_(0,var)\n",
    "        self.weight2 = nn.parameter.Parameter(torch.FloatTensor(args.hidden_size_1, args.hidden_size_2))\n",
    "        var2 = 2./(self.weight2.size(1)+self.weight2.size(0))\n",
    "        self.weight2.data.normal_(0,var2)\n",
    "        if bias:\n",
    "            self.bias = nn.parameter.Parameter(torch.FloatTensor(args.hidden_size_1))\n",
    "            self.bias.data.normal_(0,var)\n",
    "            self.bias2 = nn.parameter.Parameter(torch.FloatTensor(args.hidden_size_2))\n",
    "            self.bias2.data.normal_(0,var2)\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.fc1 = nn.Linear(args.hidden_size_2, args.num_classes)\n",
    "        \n",
    "    def forward(self, X): ### 2-layer GCN architecture\n",
    "        X = torch.mm(X, self.weight)\n",
    "        if self.bias is not None:\n",
    "            X = (X + self.bias)\n",
    "        X = F.relu(torch.mm(self.A_hat, X))\n",
    "        X = torch.mm(X, self.weight2)\n",
    "        if self.bias2 is not None:\n",
    "            X = (X + self.bias2)\n",
    "        X = F.relu(torch.mm(self.A_hat, X))\n",
    "        return self.fc1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"Testing_Data_2_ (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
